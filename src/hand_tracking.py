# -*- coding: utf-8 -*-
"""Hand Tracking.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZtXgTGyf9ft87B0jJ_FBpRDCl0cIFexc

### Environment Setup


1.   install dependencies (torch, torchvision, etc.)
2.   git clone sam2
3.   install sam2
4.   extract JPEG frames from sample video
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install -q sam2

!pip3 install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

!pip install -q mediapipe

!pip install -q opencv-python

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/

!git clone https://github.com/facebookresearch/segment-anything-2.git

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/segment-anything-2/

!pip install -e .

# Commented out IPython magic to ensure Python compatibility.
# %cd checkpoints/

!bash download_ckpts.sh

# Commented out IPython magic to ensure Python compatibility.

# %cd /content/drive/MyDrive/segment-anything-2/

# output_dir = "/content/drive/MyDrive/extracted_frames_png"
# !ffmpeg -i "$video_sample" "$output_dir/%04d.png"

video_sample = "/content/drive/MyDrive/test.mp4"
output_dir = "/content/drive/MyDrive/extracted_frames"

!ffmpeg -i "$video_sample" -q:v 2 -start_number 0 "$output_dir/%04d.jpg"

"""### Hand Tracking"""

import os
import numpy as np
import torch
import matplotlib.pyplot as plt
from PIL import Image

# select the device for computation
if torch.cuda.is_available():
  device = torch.device("cuda")
  print('device: cuda')
else:
  device = torch.device("cpu")
  print('cuda not available')

!pwd

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/segment-anything-2/

import torch
from sam2.build_sam import build_sam2_video_predictor

checkpoint = "./checkpoints/sam2.1_hiera_large.pt"
model_cfg = "configs/sam2.1/sam2.1_hiera_l.yaml"
predictor = build_sam2_video_predictor(model_cfg, checkpoint, device=device)

video_dir = "../extracted_frames/"

# scan all the JPEG frame names in this directory
frame_names = [
    p for p in os.listdir(video_dir)
    if os.path.splitext(p)[-1] in [".jpg", ".jpeg", ".JPG", ".JPEG"]
]
frame_names.sort(key=lambda p: int(os.path.splitext(p)[0]))

# take a look the first video frame
frame_idx = 0
plt.figure(figsize=(9, 6))
plt.title(f"frame {frame_idx}")
plt.imshow(Image.open(os.path.join(video_dir, frame_names[frame_idx])))

# initialize inference state
# During initialization, it loads all the JPEG frames in video_path and stores their pixels in inference_state
inference_state = predictor.init_state(video_path=video_dir)

def show_mask(mask, ax, obj_id=None, random_color=False):
    if random_color:
        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)
    else:
        cmap = plt.get_cmap("tab10")
        cmap_idx = 0 if obj_id is None else obj_id
        color = np.array([*cmap(cmap_idx)[:3], 0.6])
    h, w = mask.shape[-2:]
    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)
    ax.imshow(mask_image)


def show_points(coords, labels, ax, marker_size=200):
    pos_points = coords[labels==1]
    neg_points = coords[labels==0]
    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)
    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)


def show_box(box, ax):
    x0, y0 = box[0], box[1]
    w, h = box[2] - box[0], box[3] - box[1]
    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0, 0, 0, 0), lw=2))

ann_frame_idx = 0  # the frame index we interact with
ann_obj_id = 1  # give a unique id to each object we interact with (it can be any integers)

# Let's add a positive click at (x, y) = (580, 250) to get started
points = np.array([[580, 250]], dtype=np.float32)
# for labels, `1` means positive click and `0` means negative click
labels = np.array([1], np.int32)
_, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(
    inference_state=inference_state,
    frame_idx=ann_frame_idx,
    obj_id=ann_obj_id,
    points=points,
    labels=labels,
)

# show the results on the current (interacted) frame
plt.figure(figsize=(9, 6))
plt.title(f"frame {ann_frame_idx}")
plt.imshow(Image.open(os.path.join(video_dir, frame_names[ann_frame_idx])))
show_points(points, labels, plt.gca())
show_mask((out_mask_logits[0] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_ids[0])

ann_frame_idx = 0  # the frame index we interact with
ann_obj_id = 1  # give a unique id to each object we interact with (it can be any integers)

# Let's add a 2nd positive click at (x, y) = (780, 250) to refine the mask
# sending all clicks (and their labels) to `add_new_points_or_box`
points = np.array([[580, 250], [750, 250]], dtype=np.float32)
# for labels, `1` means positive click and `0` means negative click
labels = np.array([1, 1], np.int32)
_, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(
    inference_state=inference_state,
    frame_idx=ann_frame_idx,
    obj_id=ann_obj_id,
    points=points,
    labels=labels,
)

# show the results on the current (interacted) frame
plt.figure(figsize=(9, 6))
plt.title(f"frame {ann_frame_idx}")
plt.imshow(Image.open(os.path.join(video_dir, frame_names[ann_frame_idx])))
show_points(points, labels, plt.gca())
show_mask((out_mask_logits[0] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_ids[0])

ann_frame_idx = 0  # the frame index we interact with
ann_obj_id = 1  # give a unique id to each object we interact with (it can be any integers)

# Let's add a negative click at (x, y) = (780, 250) to refine the mask
# sending all clicks (and their labels) to `add_new_points_or_box`
points = np.array([[630, 110]], dtype=np.float32)
# for labels, `1` means positive click and `0` means negative click
labels = np.array([0], np.int32)
_, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(
    inference_state=inference_state,
    frame_idx=ann_frame_idx,
    obj_id=ann_obj_id,
    points=points,
    labels=labels,
)

# show the results on the current (interacted) frame
plt.figure(figsize=(9, 6))
plt.title(f"frame {ann_frame_idx}")
plt.imshow(Image.open(os.path.join(video_dir, frame_names[ann_frame_idx])))
show_points(points, labels, plt.gca())
show_mask((out_mask_logits[0] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_ids[0])

# run propagation throughout the video and collect the results in a dict
video_segments = {}  # video_segments contains the per-frame segmentation results
for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):
    video_segments[out_frame_idx] = {
        out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()
        for i, out_obj_id in enumerate(out_obj_ids)
    }

# render the segmentation results every few frames
vis_frame_stride = 30
plt.close("all")
for out_frame_idx in range(0, len(frame_names), vis_frame_stride):
    plt.figure(figsize=(6, 4))
    plt.title(f"frame {out_frame_idx}")
    plt.imshow(Image.open(os.path.join(video_dir, frame_names[out_frame_idx])))
    for out_obj_id, out_mask in video_segments[out_frame_idx].items():
        show_mask(out_mask, plt.gca(), obj_id=out_obj_id)

output_folder = "../masked_frames"
os.makedirs(output_folder, exist_ok=True)

# Save the segmentation results for all frames
for out_frame_idx in range(len(frame_names)):
    frame_path = os.path.join(video_dir, frame_names[out_frame_idx])
    frame_image = Image.open(frame_path)

    fig, ax = plt.subplots(figsize=(6, 4))
    ax.axis("off")
    ax.imshow(frame_image)

    if out_frame_idx in video_segments:  # Ensure segmentation data exists
        for out_obj_id, out_mask in video_segments[out_frame_idx].items():
            show_mask(out_mask, ax, obj_id=out_obj_id)  # Apply segmentation mask

    # Save the segmented frame
    segmented_frame_path = os.path.join(output_folder, f"masked_frame_{out_frame_idx:04d}.png")
    fig.savefig(segmented_frame_path, bbox_inches="tight", pad_inches=0)
    plt.close(fig)  # Close figure to free memory

print(f"Masked frames saved in {output_folder}")

# make video from masked images
import cv2
import glob

image_files = sorted(glob.glob(os.path.join(output_folder, "masked_frame_*.png")))

# Read the first frame to get dimensions
first_frame = cv2.imread(image_files[0])
height, width, layers = first_frame.shape

output_video_path = "../masked_frames/demo_video.mp4"
fourcc = cv2.VideoWriter_fourcc(*"mp4v")
fps = 30
video_writer = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))

# Write each image to the video
for image_file in image_files:
    frame = cv2.imread(image_file)
    video_writer.write(frame)

video_writer.release()
print(f"Video saved as {output_video_path}")



